\section{Evaluations}
\label{evaluation}

In this section, we show the results of experimental evaluation of decentralized search. We first give an overview of the datasets and introduce the experiment settings of our evaluations. The quality of approximated path generated by decentralized search is evaluated from two aspects, distance accuracy and path diversity, in~\ref{eval_accuracy} and~\ref{eval_diversity} respectively. We then show both overhead of index and online search in~\ref{eval_overhead}. In the last, we study the scalability of our system in~\ref{eval_scalability}.

\subsection{Datasets}
\label{eval_datasets}

\begin{table}
		\caption{Datasets}
		\vspace{2 mm}
		\label{table:datasets}
		\begin{threeparttable}
			\centering
			\begin{tabular}{c|cccc} \hline
				Dataset & Type & $|V_{wcc}|$ & $|E_{wcc}|$ & $\overline{\sigma}$ \\ \hline
				Wiki & Communication & 2.4M & 4.7M & 3.9 \\ 
				Skitter & Internet & 1.7M & 11.1M & 5.07 \\ 
				Livejournal & Social & 4.8M & 43.4M & 5.6 \\ 
				Hollywood & Collaboration & 1.1M & 56.3M & 3.83 \\ 
				Orkut & Social & 3M & 117M & 4.21 \\ 
				Sinaweibo & Social & 58.7M & 261.3M & 4.15 \\ 
				Webuk & Web & 39.3M & 796.4M & 7.45 \\ 
				Friendster & Social & 65M & 1.8B & 5.03 \\ \hline
			\end{tabular}
			\begin{tablenotes}
				\item Datasets with number of vertices and edges in the largest weakly connected components, and average shortest distance $\overline{\sigma}$ of 100,000 vertex pairs.
			\end{tablenotes}
		\end{threeparttable}
\end{table}

We evaluate our algorithm on 8 graphs from different disciplines as shown in table~\ref{table:datasets}. All graphs are complex networks that have power-law degree distribution and relatively small diameter. To simplify our experiments, we treat each graph as undirected, un-weighted graphs. We only use the largest weakly connected component of each graph which consist of more than $90\%$ of vertices for all graphs. All datasets are collected from Snap~\cite{snapnets} and NetworkRepository~\cite{nr}.

\subsection{Experiment settings}
\label{eval_system}

We evaluate our algorithms in both distributed setting and centralized setting. For the distributed setting, we use 20 Amazon EC2 m4.xlarge virtual machines. Each virtual machine has 4 vCPUs and 16 GB memory. For centralized version, we use a Cloudlab~\cite{RicciEide:login14} c8220 server with two 10-core 2.2GHz E5-2660 processors and 256GB memory. Powergraph~\cite{180251} is the platform for distributed version and Snap~\cite{snapnets} is the platform for centralized version. All algorithms are implemented in C++. 

All the evaluations use the same landmark selection strategy, a variation based on $DEGREE/h$ strategy from reference~\cite{Potamias:2009:FSP:1645953.1646063}. When selecting a new landmark, each vertex receive a rank which is the product of its degree and the sum of distance to all the existing landmarks. The vertex with highest rank will be added to the landmark set. We denote number of landmarks as $k$.

Queries in our experiments are randomly generated. For accuracy evaluations, since performing BFS on large graphs is extremely slow, we randomly choose 1,000 vertices of each graph as source vertices and randomly pick 100 target vertices for each source vertex. All results in~\ref{eval_accuracy} are averaged among 100,000 queries.

\subsection{Approximation Accuracy}
\label{eval_accuracy}

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{./figures/accuracy_stepy.pdf}
    \caption{The accuracy of the approximated distance of decentralized search and various optimizations compared to TreeSketch.}
    \label{fig:accuracy_stepy}
\end{figure*}

We first evaluate the approximation accuracy of the decentralized search. We use the average approximation error as the measure of accuracy which is defined as follows:
\[
E_{\tilde{p}(s,t)} = \frac{|\tilde{p}(s,t)| - d_G(s,t)}{d_G(s,t)}
\]
We show the results of 4 variants of decentralized search with mixture of different tie breaking strategy and index construction approach. All the decentralized search are performed with bidirectional search. We also list the performance of an state-of-the-art existing method, TreeSketch~\cite{Gubichev:2010:FAE:1871437.1871503}. We vary the size of landmark sets and show the averaged results in Fig.~\ref{fig:accuracy_stepy}.
%[Reference~\cite{6399472} also includes a online search method but is quite similar to TreeSketch in nature so that we haven't included here.]

We can see in Fig.~\ref{fig:accuracy_stepy} that decentralized search achieves better accuracy in most of cases. Especially with small landmark sets, i.e. $k < 5$, decentralized search outperforms TreeSketch on all the graphs. When the full branch decentralized search are carried on the index constructed by our greedy heuristic, the performance gain is most noticeable, with 43.3\% to 87.7\% lower average error ratio for 1 landmark and 50\% to 80\% lower average error ratio for 20 landmarks than TreeSketch on all graphs.

Full branch tie strategy always outperforms single branch tie strategy with large margins with same landmark sets. The average error ratio of full branch search with regular index is 17.4\% to 45.7\% lower than single branch for 1 landmark and 19.5\% to 61.8\% lower with 20 landmarks on various graphs. As the number of landmark increases, the performance gain also increase.   

Decentralized search carried on index constructed by greedy heuristic has lower error ratio than decentralized search with regular index. The average error ratio is 14.5\% to 60.2\% lower for single branch and 21.1\% to 63.3\% lower for full branch for 1 landmark. For 20 landmarks, the search is 10.4\% to 68.8\% lower for single branch and 12.8\% to 75\% lower for full branch.

\subsection{Path Diversity}
\label{eval_diversity}

\begin{table}
	\caption{Path diversity (k = 2)}
    \label{table:pdiv}
    \centering
    \begin{tabular}{c|ccc} \hline
				Graph&Path cnt(DS)&Path cnt(TS)& $\overline{r_p}$ \\ \hline
				Wiki&28.9&1.9&0.372 \\ 
				Skitter&24.1&2.4&0.418 \\ 
				Livejournal&30.8&1.9&0.338 \\ 
				Hollywood&9.9&2.6&0.471 \\ 
				Orkut&19.2&3.2&0.465 \\ 
				Sinaweibo&32.0&3.0&0.301 \\ 
				Webuk&704.1&2.0&0.501 \\ 
				Friendster&16.8&2.8&0.39 \\ \hline
    \end{tabular}
\end{table}

We show in this section that decentralized search achieves better path diversity by finding more paths and not being constrained by the index. Table~\ref{table:pdiv} shows average number pf paths with shortest approximated distance returned by decentralized search with full branch tie strategies compared to TreeSketch. The average path count of full branch decentralized search is much higher than that of TreeSketch, from 3.73 to 345.13 times for various graphs.

Moreover, decentralized search is not restricted by label of source and target vertices. We define the ratio $r_p$ the number of vertices not in label source and target compared to total vertices except source and target:
\[
r_p(s,t) = \frac{|\{u:u \in p(s,t), u \notin L(s) \cup L(t)\}|}{|{v:v \in p(s,t), v \neq s, v \neq t}|}
\]
The higher the $r_p$'s value, the lower the dependence of a path to label of source and target vertices. As shown in Table~\ref{table:pdiv}, the average $r_p$ for full branch decentralized search on various graphs ranges from 0.301 to 0.501.

\subsection{Overhead}
\label{eval_overhead}

\begin{table}
		\caption{Index overhead per landmark}
    \label{table:ioh}
    \centering
    \begin{tabular}{c|cc} \hline
				Graph&Index size(MB)&Query size(Byte)\\ \hline
				Wiki&189.9&369.8 \\ 
				Skitter&143.7&412.9 \\ 
				Livejournal&429.4&419.9 \\ 
				Hollywood&84.1&377.5 \\ 
				Orkut&246.3&390.7 \\ 
				Sinaweibo&4313.8&367.4 \\ 
				Webuk&4068.9&506.5 \\ 
				Friendster&5497.7&437.7 \\ \hline
    \end{tabular}
\end{table}

The index and query overhead is shown in table~\ref{table:ioh}. In our implementation, both the label for each vertex and approximated paths are stored as vector of C++ standard library. And each vertex id is represented by 8-byte unsigned long. The size shown in table~\ref{table:ioh} is the sum of vector size of each vertex. 

\begin{figure*}[t]
    \centering
    \includegraphics[width=\linewidth]{./figures/overhead_search.pdf}
    \caption{The amortized search time of decentralized search with various optimizations.}
    \label{fig:overhead_search}
\end{figure*}

Fig.~\ref{fig:overhead_search} shows the online search overhead in log scale for both non-parallel mode and parallel batch mode. By examing all the neighbors of a vertex at each step, decentralized search introduces much higher online search overhead than TreeSketch. Nevertheless, due to that decentralized search has very low foot print, large amount of searches can run independently in parallel efficiently. We can see in Fig.~\ref{fig:overhead_search}, the parallel mode perform 100,000 queries simultaneously, which lead to very low amortized search time for each query. The amortized search time is reduced to from 0.18\% to 1.1\% for single branch and from 0.15\% to 1.3\% for full branch compared to non-parallel mode. 

For both non-parallel and parallel mode, full branch tie break strategy introduces much higher overhead than single branch strategy. Ranging from 3.3 to 30.8 times higher search time for non-parallel version and from 5.1 to 44.4 times higher average search time for parallel version.

\subsection{Scalability}
\label{eval_scalability}

Since our algorithm is designed for large-scale networks, scalability is another major concern of our algorithm. Due to that we implement the algorithm in a distributed setting and execute queries in a parallel way, we study how our algorithm performs when number of machines and queries increase. We only perform single branch decentralized search here as full branch search is equal to multiple independent single branch searches.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{./figures/scale_machine.pdf}
    \caption{The total run time for 1 million queries as the number of machines increase}
    \label{fig:scale_machine}
\end{figure}

We first evaluate the search time when number of machines increasing. Results shown in Fig.~\ref{fig:scale_machine} are averaged over 1,000,000 queries. We can see the trend is that the average run-time decreases as the number of machines increase. The average search time decreases fast when number of machines is small and slower when large number of machines are used. The total run time on 16 machines range from 17\% to 33\% of the total run time on a single machine for various graphs.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{./figures/scale_query.pdf}
    \caption{The total run time on 20 machines as the number of queries increase}
    \label{fig:scale_query}
\end{figure}

We observe great scalability of decentralized search as number of queries increase. All experiments are carried on 20 machines in this section. We can see in Fig.~\ref{fig:scale_query} that the total search time increases slowly when the number of queries is small. Because for small number of queries, constant overheads such as engine start/stop is the major part of run time. For large number of queries, the total run time growth is still lower than linear growth. The growth of total search time never catch up with the growth of number of queries until it reaches the system limit, i.e. memory limit or network limit.

%\begin{figure}[t]
    %\centering
    %\includegraphics[width=\linewidth]{./figures/scale_graph.pdf}
    %\caption{The average search time for different size of graphs}
    %\label{fig:scale_graph}
%\end{figure}
%
%In the last experiment, we evaluate the search time as the size of graph increases. Unlike BFS which needs to traverse neighbors of $O(n)$ vertices, decentralized search only needs to exam neighbors of $O(log(n))$ vertices for complex networks due to the small world property. We can clearly see in Fig. \ref{fig:scale_graph} that as the number of vertices increases, average search time of decentralized search does not follow the same trend. The graph friendster which have sixty-five millions vertices have average search time only $2.05$ times more than graph mouse-gene which only have forty-three thousand vertices.

%\subsection{error distribution}
%
%\begin{figure}[t]
    %\centering
    %\includegraphics[width=\linewidth]{./figures/empty.pdf}
    %\caption{Distribution of errors of different length}
    %\label{fig:distribution}
%\end{figure}
%
%\comment{we want to show that the algorithm performs equally well for various path length}
%In the last, we study the error distribution according to the length of exact shortest path.
