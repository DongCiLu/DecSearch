\section{Distance Calculations}
\label{distance}

As we discussed in previous section, applications of point-to-point shortest path problem usually solve large amount of queries repeatedly on same underlying network. Thus most of algorithms in this area allow preprocessing to speed up following online queries. A distance oracle (centralized) or distance labels (distributed) will be generated after preprocessing that can quickly answer shortest distance queries. Time and space for preprocessing usually should be limited to linear in the graph size with small constant \cite{Goldberg:2005:CSP:1070432.1070455}. For example, for a graph with several millions of vertices, typical preprocessing take seconds to thousands of seconds to finish, and consume hundreds of Megabytes to several Gigabytes of storage. With the help of data structures generated by preprocessing, shortest distance queries can be answered in microseconds and shortest path queries can be answered in milliseconds for network with millions of vertices compared to several seconds for online traversals without preprocessing. In this section, we discuss three fundamental preprocessing based methods that have been used by most of point-to-point shortest path algorithms for complex networks. We also introduce recent improvements and variants of these algorithms.

\subsection{Preliminaries}

In our problem, we consider a graph $G = (V,E)$, which represents a graph of vertex set $V$ and edge set $E$. For a source vertex $s$ and target vertex $t$, we are interested in finding a path $p$ in the graph, which is an ordered sequence of $n$ vertices, such that,

\[
    p = (v_0, v_1, ..., v_{n-1})
\]

where $(v_i, v_{i+1}) \in E$. We can write that $|p| = n$ to denote that length of $p$ is $n$. Given a pair of vertices, there may be many paths connecting them, which we denote as $P$. We can then define the distance between $s$ and $t$ as:

\[
    d_G(s,t) = min_{p \in P(s,t)}|p|
\]

\subsection{Landmarks}

\subsubsection{Basic Algorithm}

Landmark based preprocessing algorithms are widely used for approximate shortest distance between vertices $L$. The preprocessing first choosing a small (constant) number of vertices as landmarks. Then for each landmark, the algorithm perform a BFS/Dijkstra traversal. After this operation, each vertex in the graph will be assigned a label with distance to each vertex in the landmark set. Upper and Lower bounds can be computed in constant time using these labels according to triangle inequality as follows:

\[
    d_G(s,t) \leq min_{l \in L}\{d_G(s,l)+d_G(l,t)\}
\]
\[
    d_G(s,t) \geq max_{l \in L}|d_G(s,l)-d_G(l,t)|
\]

where $d_G(s,l)$ and $d_G(l,t)$ are stored in the labels of $s$ and $t$. 

On the theoretic side, Thorup and Zwick proved that for any integer $k\geq1$, with $O(kmn^{1/k})$ expected preprocessing time and $O(kn^{1+1/k})$ storage size, and a point-to-point shortest path query can be answered in $O(k)$ time \cite{Thorup:2005:ADO:1044731.1044732}. The ratio of estimated distance and the exact should lie within $[1,2k-1]$. For k = 1, we are actually computing APSP and store the results, which is very expensive as discussed in previous section. For k = 2, the estimate may be three times larger, that is unacceptable due to most complex networks has the small-world property. Although theoretical bounds of landmark based algorithms is not very promising, they works well in practice for large-scale complex networks \cite{Sommer:2014:SQS:2597757.2530531}. Intuitively, labels of vertices encapsulates shortest path distance to the landmarks, which is part of the actual topology of the network, leading to accurate distance estimates.

\subsubsection{Variance of Landmark based preprocessing}

The choice of the landmark sets are important to give the distance estimations, M. Potamias et al. studied various strategies for choosing landmarks \cite{Potamias:2009:FSP:1645953.1646063}. It turned out that the optimal landmark selection problem is NP-hard. Several landmark selection heuristics and different ways to use triangle inequality bound as distance estimates were evaluated in their paper. Although several advanced landmark selection strategies have better performances, simple heuristics such as choosing landmarks based on degree achieve high performance and low computation overhead at same time, that make them popular in this field.

The distance estimates of landmark approach heavily depends on the quality of label on each vertex, the more information label carries the better estimation results will be. In order to maximize information stored in each label, Akiba et al. proposed to doing a BFS from every vertex in the network \cite{Akiba:2013:FES:2463676.2465315}. The key contribution is that to reduce the large time and storage overhead brought by the huge number of BFS to an acceptable level, they prune during each BFS. The algorithm doing BFS in an iterative way, when previous BFS already explored a shortest path from current root to the visiting vertex, then this vertex will not be pruned. For example, suppose the algorithm already have an index $L_{k-1}$ and is doing a BFS from $v_k$. Assuming this BFS reach a vertex $u$ with distance $\delta$. IF $d_{L_{k-1}}(v_k,u) = \delta$, the algorithm prune $u$, that is, $(v_k,\delta)$ will not be added to $L_k$ as well as it will not be added to the priority queue of current BFS. Without all edge information incorporated into labels, this algorithm can calculate exact shortest distance instead of approximations. Even with pruning during BFS, the algorithm still take much longer time to preprocessing and have a relatively large label size.

Instead of only storing shortest path distance to each landmark as labels, Maier et al. stored shortest paths to landmarks for each vertex, they called it network structure indexes or NSI \cite{Maier:2011:INS:1993077.1993079}. They use lowest common ancestor to further lower the upper bound given by triangle inequality by finding the smallest triangle along the path. In this way, not only the NSI improves the distance estimates, it can also return the related path without performing any online searching. The obvious drawback of this algorithm is that the size of the label is increased. 

Landmark based algorithms have a well known drawback, that is, it is not good at approximating distance for two vertices near each other. To solve this problem, Qiao et al. proposed a local landmark scheme that choose landmarks based on each query \cite{6399472}.

\textbf{Graph Embedding Methods:} The embedding methods, which aim to transform graphs into alternative representations. By doing so, the algorithm can obtain significant speed-ups by embedding vertices into a different space and using a more efficient distance functions. Zhao et al. take a quiet different approach to estimates distance between vertices, they tried to map vertices in high dimensional graphs to positions in low-dimension coordinate spaces so that distance estimates between vertices can be easily calculated \cite{Zhao:2010:OSP:1863190.1863199}, \cite{DBLP:conf/colcom/ZhaoSZZ11}. To avoid large number of pairwise distance calculations, the algorithm first choose a small set of landmarks, and use global optimization algorithm to fix their coordinates. Then the algorithm add vertices to the coordinate space according to their distance to these landmarks in an iterative way. Note that although this algorithm use a different way than regular landmark based algorithms, it still use the same information, so the quality of distance is still the same as landmark based algorithms.

\subsection{2-hop Cover}

\subsubsection{Basic Algorithm}

While landmark based algorithms have been widely used to estimate shortest path distance, 2-hop cover is a different preprocessing algorithm that aim to finding exact shortest path distance. 

The idea of 2-hop cover is pretty simple. For each vertex $u$, we assign a set $C(u)$ of vertices so that every pair of vertices $(u,v)$ has at least one vertex $w \in C(u) \cap C(v)$ on a shortest path between the pair. The distance from each vertex to each vertices in its cover composed its label $L(u) = {(w, d_G(u,w))}_{w \in C(u)}$. It is easy to calculate distance from $s$ to $t$ with their labels $L(s)$, $L(t)$ using following formula:

\[
    d_G(s,t) = min\{\delta + \delta' | (s,\delta) \in L(u), (w,\delta') \in L(v)\}
\]

The labels ${L(u)}$ is called a 2-hop cover. Although the idea of 2-hop cover is straight forward, but finding optimal 2-hop cover is a very challenging problem \cite{Cohen:2002:RDQ:545381.545503}.

\subsubsection{Variances of 2-hop Cover}

Agarwal et al. use a similar approach as 2 hop cover \cite{Agarwal:2012:SPL:2342549.2342559}. In the preprocessing period, a subset of neighbor vertices was stored as vicinity of each vertex. During online searching period, algorithms searching intersects between vicinities of two vertices. Each intersect vicinity lies in a path between source vertex and target vertex. The algorithm will compare each path's length and return the shortest one as the estimation of shortest path between the vertex pair. 

Highway structure has been widely used to facilitate finding shortest path in road networks. Intuitively for road networks any shortest path longer than certain hops contain a highway vertex. Jin et al. explore this highway structure in large sparse graphs, the algorithm build a tree structured highway to connect source vertex and target vertex together \cite{Jin:2012:HLA:2213836.2213887}. This algorithm can be seen as a generalization of 2 hop cover, since it use a intermediate tree instead of a single vertex.

\subsection{Tree Decomposition}

\subsubsection{Basic Algorithm}

Tree decomposition based algorithms are another family of algorithms that finding exact shortest path distances. A tree decomposition of $G$ is a pair $(T,X)$, where $T$ is a tree and $X = \{X_t|t \in V(T)\}$ is a family of subsets of $V(G)$, which are called bags, with the following properties: $(i) U_{t \in V(T)}X_t = V(G)$. $(ii)$ For every $(u,v) \in E(G)$, there exists $t\ \in V(T)$ such that $u,v \in X_t$. $(iii)$ For all $v \in V(G)$, the set $\{t|v \in X_t\}$ induces a subtree of T. The number of bags is denoted as $b$. The width of a tree decomposition $(T, X)$ is $max_{t \in V(T)}\{|X_t|-1\}$ which is refer to as $w$.

During preprocessing, the distance between any two vertices $u,v$ contained in the same bag has been computed and stored as labels. 

Let $T_s$ and $T_t$ be the subtrees of $T$ induced by the bags including $s$ and $t$. Let $t_s$ and $t_t$ be the vertices of $T_s$ and $T_t$ that is closest to the root of $T$. Let $u$ be the lowest common ancestor of $t_s$ and $t_t$. Any path from $s$ to $t$ must contain at least one vertex in the bag $X_u$. So the distance between $s$ and $t$ can be computed from the following equation:

\[
    d_G(s,t) = min_{v \in X_u}\{d_G(s,v)+d_G(v,t)\}
\]

The $d_G(s,t)$ can be derived from this fact based on dynamic programming based on the labels created during preprocessing.

Since only the distance of every vertex pairs in same bags is computed and stored during preprocessing, so the overhead depends on the tree width of the tree decomposition of the network.

\subsubsection{Tree decomposition based on core-fringe structure}

Emerging in the area of finding shortest path in road networks, original tree decomposition based algorithms does not perform well on complex networks since the tree width is quite large which lead to unacceptable storage overheads. Akiba et al. leverage the core-fringe structure that most complex network shared \cite{Akiba:2012:SQC:2247596.2247614}, that is, most complex network has a relatively dense core and leaves a tree-like fringe. With this structure property, the tree width of complex networks outside the dense core is actually very small thus suitable for tree decomposition based algorithms.  

\subsection{Comparison}

\begin{table*}
    \centering
    \begin{tabular}{|c|c|c|c|c|} \hline
        Algorithm family & return type & computation overhead & storage overhead & query time \\ \hline
        Landmark & approximate & $O(|L||E|)$ & $O(|L||V|)$ & $O(1)$ \\ \hline
        2-hop Cover & exact & no theoretical bound & $O(|V|\sqrt|E|)$$\star$ & $O(1)$ \\ \hline
        Tree Decomposition & exact & $O(|V||E|+bw^2)$ & $O(|E|)$ & $(w^5log^3|V|)$ \\ \hline
    \end{tabular}
    \begin{tablenotes}
        \small
        \item $\star$ This is an approximated optimal value from \cite{Cohen:2002:RDQ:545381.545503}.
    \end{tablenotes}
    \caption{Comparison of preprocessing algorithms}
    \label{table:comparison}
\end{table*}

Table~\ref{table:comparison} compared the three algorithm families and highlight their limitations on large-scale complex networks. Although landmark based algorithm only return estimated results, due to its simplicity and lower computation and storage overhead, it is the most popular preprocessing algorithm in the literature. On the contrary, 2-hop cover and tree decomposition have larger overhead but can return exact results. Since the optimal size of 2-hop cover has not yet been solved, so there are no theoretical bound on the overheads. For tree decomposition, even some algorithms take advantage of the core-fringe structure that most complex networks have, the non-constant query time still make it less practical for applications with huge number of shortest path/distance queries.
