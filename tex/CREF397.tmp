\section{Evaluations}
\label{evaluation}

In this section, we show the results of experimental evaluation of decentralized search. We first give an overview of the datasets and introduce the experiment settings of our evaluations. The quality of approximated path generated by decentralized search is evaluated from two aspects, distance accuracy and path diversity, in~\ref{eval_accuracy} and~\ref{eval_diversity} respectively. We then show both overhead of index and online search in~\ref{eval_overhead}. In the last, we study the scalability of our system in~\ref{eval_scalability}.

\subsection{Datasets}
\label{eval_datasets}

\begin{table}
		\caption{Datasets}
		\vspace{2 mm}
		\label{table:datasets}
		\begin{threeparttable}
			\centering
			\begin{tabular}{c|cccc} \hline
				Dataset & Type & $|V_{wcc}|$ & $|E_{wcc}|$ & $\overline{\sigma}$ \\ \hline
				Wiki & Communication & 2.4M & 4.7M & 3.9 \\ 
				Skitter & Internet & 1.7M & 11.1M & 5.07 \\ 
				Livejournal & Social & 4.8M & 43.4M & 5.6 \\ 
				Hollywood & Collaboration & 1.1M & 56.3M & 3.83 \\ 
				Orkut & Social & 3M & 117M & 4.21 \\ 
				Sinaweibo & Social & 58.7M & 261.3M & 4.15 \\ 
				Webuk & Web & 39.3M & 796.4M & 7.45 \\ 
				Friendster & Social & 65M & 1.8B & 5.03 \\ \hline
			\end{tabular}
			\begin{tablenotes}
				\item Datasets with number of vertices and edges in the largest weakly connected components, and average shortest distance $\overline{\sigma}$ of 100,000 vertex pairs.
			\end{tablenotes}
		\end{threeparttable}
\end{table}

We evaluate our algorithm on 8 graphs from different disciplines as shown in table~\ref{table:datasets}. All graphs are complex networks that have power-law degree distribution and relatively small diameter. To simplify our experiments, we treat each graph as undirected, un-weighted graphs. We only use the largest weakly connected component of each graph which consist of more than $90\%$ of vertices for all graphs. All datasets are collected from Snap~\cite{snapnets} and NetworkRepository~\cite{nr}.

\subsection{Experiment settings}
\label{eval_system}

We evaluate our algorithms in both distributed setting and centralized setting. For the distributed setting, we use 20 Amazon EC2 m4.xlarge virtual machines. Each virtual machine has 4 vCPUs and 16 GB memory. For centralized version, we use a Cloudlab~\cite{RicciEide:login14} c8220 server with two 10-core 2.2GHz E5-2660 processors and 256GB memory. Powergraph~\cite{180251} is the platform for distributed version and Snap~\cite{snapnets} is the platform for centralized version. All algorithms are implemented in C++. 

All the evaluations use the same landmark selection strategy, a variation based on $DEGREE/h$ strategy from reference~\cite{Potamias:2009:FSP:1645953.1646063}. When selecting a new landmark, each vertex receive a rank which is the product of its degree and the sum of distance to all the existing landmarks. The vertex with highest rank will be added to the landmark set. We denote number of landmarks as $k$.

Queries in our experiments are randomly generated. For accuracy evaluations, since performing BFS on large graphs is extremely slow, we randomly choose 1,000 vertices of each graph as source vertices and randomly pick 100 target vertices for each source vertex. All results in~\ref{eval_accuracy} are averaged among 100,000 queries.

\subsection{Approximation Accuracy}
\label{eval_accuracy}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=\linewidth]{./figures/accuracy_stepy.pdf}
    \caption{The accuracy of the approximated distance of decentralized search and various optimizations compared to TreeSketch.}
    \label{fig:accuracy_stepy}
\end{figure*}

We first evaluate the approximation accuracy of the decentralized search. We use the average approximation error as the measure of accuracy which is defined as follows:
\[
E_{\tilde{p}(s,t)} = \frac{|\tilde{p}(s,t)| - d_G(s,t)}{d_G(s,t)}
\]
We show the results of 4 variants of decentralized search with mixture of different tie breaking strategy and index construction approach. All the decentralized search are performed with bidirectional search. We also list the performance of an state-of-the-art existing online search method, TreeSketch~\cite{Gubichev:2010:FAE:1871437.1871503}. We vary the size of landmark sets and show the averaged results in Fig.~\ref{fig:accuracy_stepy}.
%[Reference~\cite{6399472} also includes a online search method but is quite similar to TreeSketch in nature so that we haven't included here.]

We can see in Fig.~\ref{fig:accuracy_stepy} that decentralized search achieves better accuracy in most of cases. Especially with small landmark sets, i.e. $k < 5$, decentralized search outperforms TreeSketch on all the graphs. When the full branch decentralized search are carried on the index constructed by our greedy heuristic, the performance gain is most noticeable, with 43.3\% to 87.7\% lower average error ratio for 1 landmark and 50\% to 80\% lower average error ratio for 20 landmarks than TreeSketch on all graphs.

Full branch tie strategy always outperforms single branch tie strategy with large margins with same landmark sets. The average error ratio of full branch search with regular index is 17.4\% to 45.7\% lower than single branch for 1 landmark and 19.5\% to 61.8\% lower with 20 landmarks on various graphs. As the number of landmark increases, the performance gain also increase.   

Decentralized search carried on index constructed by greedy heuristic has lower error ratio than decentralized search with regular index. The average error ratio is 14.5\% to 60.2\% lower for single branch and 21.1\% to 63.3\% lower for full branch for 1 landmark. For 20 landmarks, the search is 10.4\% to 68.8\% lower for single branch and 12.8\% to 75\% lower for full branch.

\subsection{Path Diversity}
\label{eval_diversity}

\begin{table}
	\caption{Path diversity (k = 2)}
    \label{table:pdiv}
    \centering
    \begin{tabular}{c|ccc} \hline
				Graph&Path cnt(DS)&Path cnt(TS)& $\overline{r_p}$ \\ \hline
				Wiki&28.9&1.9&0.372 \\ 
				Skitter&24.1&2.4&0.418 \\ 
				Livejournal&30.8&1.9&0.338 \\ 
				Hollywood&9.9&2.6&0.471 \\ 
				Orkut&19.2&3.2&0.465 \\ 
				Sinaweibo&32.0&3.0&0.301 \\ 
				Webuk&704.1&2.0&0.501 \\ 
				Friendster&16.8&2.8&0.39 \\ \hline
    \end{tabular}
\end{table}

We show in this section that decentralized search achieves better path diversity by finding more paths and not being constrained by the index. Table~\ref{table:pdiv} shows average number pf paths with shortest approximated distance returned by decentralized search with full branch tie strategies compared to TreeSketch. The average path count of full branch decentralized search is much higher than that of TreeSketch, from 3.73 to 345.13 times for various graphs.

Moreover, decentralized search is not restricted by label of source and target vertices. We define the ratio $r_p$ the number of vertices not in label source and target compared to total vertices except source and target:
\[
r_p(s,t) = \frac{|\{u:u \in p(s,t), u \notin L(s) \cup L(t)\}|}{|{v:v \in p(s,t), v \neq s, v \neq t}|}
\]
The higher the $r_p$'s value, the lower the dependence of a path to label of source and target vertices. As shown in Table~\ref{table:pdiv}, the average $r_p$ for full branch decentralized search on various graphs ranges from 0.301 to 0.501.

\subsection{Overhead}
\label{eval_overhead}

\begin{figure*}[ht]
    \centering
    \includegraphics[width=0.95\linewidth]{./figures/throughput.pdf}
    \caption{The amortized search time of decentralized search with various optimizations.}
    \label{fig:throughput}
\end{figure*}

\begin{table}
		\caption{Space overhead}
    \label{table:ioh}
    \centering
    \begin{tabular}{c|cc} \hline
				Graph&Index size(MB/landmark)&Query size(Byte)\\ \hline
				Wiki&189.9&369.8 \\ 
				Skitter&143.7&412.9 \\ 
				Livejournal&429.4&419.9 \\ 
				Hollywood&84.1&377.5 \\ 
				Orkut&246.3&390.7 \\ 
				Sinaweibo&4313.8&367.4 \\ 
				Webuk&4068.9&506.5 \\ 
				Friendster&5497.7&437.7 \\ \hline
    \end{tabular}
\end{table}

The index and query overhead is shown in table~\ref{table:ioh}. In our implementation, both the label for each vertex and approximated paths are stored as vector of C++ standard library. And each vertex id is represented by 8-byte unsigned long. The size shown in table~\ref{table:ioh} is the sum of vector size of each vertex. 

Fig.~\ref{fig:throughput} shows the throughput of our system in log scale for both serial mode and parallel mode. The throughput of parallel mode is calculated based on running 100,000 queries simultaneously. The throughput of parallel mode is much higher than serial mode, from 94.7 to 544.4 times for single branch decentralized search and 74.7 to 679.1 times for full branch decentralized search. 

We can also see that the due to the search duplicates itself for full branch decentralized search, compared to single branch, the throughput is 3.2\% to 30.4\% for serial mode and 2.3\% to 21.1\% for parallel mode. 

\subsection{Scalability}
\label{eval_scalability}

Since our algorithm is designed for large-scale networks, scalability is another major concern of our algorithm. Due to that we implement the algorithm in a distributed setting and execute queries in a parallel way, we study how our algorithm performs when number of machines and queries increase. We only perform single branch decentralized search here as full branch search is equal to multiple independent single branch searches thus have the similar trend.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{./figures/scale_machine_throughput.pdf}
    \caption{The total run time for 1 million queries as the number of machines increase}
    \label{fig:scale_machine}
\end{figure}

We first evaluate the search time when number of machines increasing. Results shown in Fig.~\ref{fig:scale_machine} are based on the results of running 1,000,000 queries simultaneously. We can see the throughput increases as the number of machines increases. The throughput on 16 machines is 3.0 to 5.9 times higher than on a single machine for various graphs. Note that we do not have results for the 3 larger graphs as they are not able to fit into less than 8 machines.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\linewidth]{./figures/scale_query_throughput.pdf}
    \caption{The total run time on 20 machines as the number of queries increase}
    \label{fig:scale_query}
\end{figure}

We also show the trend of the throughput as number of queries running simultaneously increases. All experiments are carried on 20 machines. We can see in Fig.~\ref{fig:scale_query} the constant growth of throughput as the number of queries running simultaneously increases. The growth of throughput slow down for large number of queries as the system limits are reached, i.e. memory size or network bandwidth. The throughput for 1,600,000 queries running simultaneously is 2.3 to 5.0 times higher than for 100,000 queries for various graphs.
